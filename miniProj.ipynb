{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f962b643",
   "metadata": {},
   "source": [
    "# Mini Project - Deep Learning Course 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231bc900",
   "metadata": {},
   "source": [
    "- Download the dataset from huggingface wmt14, use the fr-en subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e6cac",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "This implementation follows \"Sequence to Sequence Learning with Neural Networks\" (Sutskever et al., 2014) for English→French machine translation on the WMT14 dataset. We use a 4-layer LSTM encoder-decoder architecture with scaled-down dimensions for computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec146653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model configuration: 4 layers, 512 hidden dim, 256 embedding dim\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sequence to Sequence Learning with Neural Networks\n",
    "Reimplementation of Sutskever et al., 2014 for English→French translation\n",
    "\n",
    "Architecture: 4-layer LSTM encoder-decoder with teacher forcing\n",
    "Dataset: WMT14 fr-en (10k train, 1k val, 1k test samples)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import sacrebleu\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "class Config:\n",
    "    \"\"\"Hyperparameters scaled down from the paper for computational constraints.\"\"\"\n",
    "    # Data\n",
    "    SEED = 42\n",
    "    TRAIN_SIZE = 10_000\n",
    "    VAL_SIZE = 1_000\n",
    "    TEST_SIZE = 1_000\n",
    "    MAX_SEQ_LEN = 50  # Truncate sequences longer than this\n",
    "    \n",
    "    # Vocabulary\n",
    "    MIN_FREQ = 2  # Minimum frequency to include in vocabulary\n",
    "    MAX_VOCAB_SIZE = 30_000\n",
    "    \n",
    "    # Model (scaled down from paper's 1000 dim for compute constraints)\n",
    "    EMBEDDING_DIM = 256\n",
    "    HIDDEN_DIM = 512\n",
    "    NUM_LAYERS = 4  # As specified in the paper\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 10\n",
    "    TEACHER_FORCING_RATIO = 0.5\n",
    "    CLIP_GRAD = 5.0  # Gradient clipping as in the paper\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = Config.SEED):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "print(f\"Using device: {Config.DEVICE}\")\n",
    "print(f\"Model configuration: {Config.NUM_LAYERS} layers, {Config.HIDDEN_DIM} hidden dim, {Config.EMBEDDING_DIM} embedding dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8220325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sacrebleu rich -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d893c37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d4461a98764a1fa146d8d881a78900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b3ace058e546ae924ae161e8e4f013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40836715</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    validation: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3000</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3003</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m40836715\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    validation: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m3000\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m3003\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Resumption of the session'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fr'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Reprise de la session'</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'translation'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'en'\u001b[0m: \u001b[32m'Resumption of the session'\u001b[0m, \u001b[32m'fr'\u001b[0m: \u001b[32m'Reprise de la session'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import rich as ri\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"wmt14\", \n",
    "    \"fr-en\",\n",
    "    cache_dir=\"./data\"\n",
    ")\n",
    "\n",
    "ri.print(raw_dataset)\n",
    "ri.print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92444396",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load WMT14 English-French dataset and create reproducible subsets:\n",
    "- 10,000 training examples\n",
    "- 1,000 validation examples  \n",
    "- 1,000 test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fe7150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 10000\n",
      "Validation examples: 1000\n",
      "Test examples: 1000\n",
      "\n",
      "Sample pair:\n",
      "  English: It should also be recalled that Australia and Japan have announced ambitious goals - not yet in binding terms, certainly, but at a political level.\n",
      "  French:  Il faut rappeler aussi que l'Australie et le Japon ont annoncé - pas encore en termes contraignants, certes, mais déjà sur un plan politique - des objectifs ambitieux.\n"
     ]
    }
   ],
   "source": [
    "# Create reproducible subsets\n",
    "train_ds = raw_dataset[\"train\"].shuffle(seed=Config.SEED).select(range(Config.TRAIN_SIZE))\n",
    "val_ds = raw_dataset[\"validation\"].shuffle(seed=Config.SEED).select(range(Config.VAL_SIZE))\n",
    "test_ds = raw_dataset[\"test\"].shuffle(seed=Config.SEED).select(range(Config.TEST_SIZE))\n",
    "\n",
    "print(f\"Training examples: {len(train_ds)}\")\n",
    "print(f\"Validation examples: {len(val_ds)}\")\n",
    "print(f\"Test examples: {len(test_ds)}\")\n",
    "print(f\"\\nSample pair:\")\n",
    "print(f\"  English: {train_ds[0]['translation']['en']}\")\n",
    "print(f\"  French:  {train_ds[0]['translation']['fr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0b2dd",
   "metadata": {},
   "source": [
    "### 2.1 Vocabulary Class\n",
    "\n",
    "Build source (English) and target (French) vocabularies with special tokens:\n",
    "- `<pad>`: Padding token\n",
    "- `<bos>`: Beginning of sequence\n",
    "- `<eos>`: End of sequence  \n",
    "- `<unk>`: Unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510827d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Word-level vocabulary with special tokens.\n",
    "    \n",
    "    Following the paper's approach: simple word-level tokenization.\n",
    "    \"\"\"\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "    BOS_TOKEN = \"<bos>\"\n",
    "    EOS_TOKEN = \"<eos>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    \n",
    "    def __init__(self, min_freq: int = 2, max_size: int = 30_000):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.word2idx: Dict[str, int] = {}\n",
    "        self.idx2word: Dict[int, str] = {}\n",
    "        self._init_special_tokens()\n",
    "        \n",
    "    def _init_special_tokens(self):\n",
    "        \"\"\"Initialize vocabulary with special tokens.\"\"\"\n",
    "        special_tokens = [self.PAD_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN, self.UNK_TOKEN]\n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "    \n",
    "    @property\n",
    "    def pad_idx(self) -> int:\n",
    "        return self.word2idx[self.PAD_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def bos_idx(self) -> int:\n",
    "        return self.word2idx[self.BOS_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def eos_idx(self) -> int:\n",
    "        return self.word2idx[self.EOS_TOKEN]\n",
    "    \n",
    "    @property\n",
    "    def unk_idx(self) -> int:\n",
    "        return self.word2idx[self.UNK_TOKEN]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def build_vocab(self, sentences: List[str]):\n",
    "        \"\"\"\n",
    "        Build vocabulary from a list of sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences (strings)\n",
    "        \"\"\"\n",
    "        counter = Counter()\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        # Sort by frequency (descending) and add to vocabulary\n",
    "        sorted_words = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        \n",
    "        for word, freq in sorted_words:\n",
    "            if freq < self.min_freq:\n",
    "                continue\n",
    "            if len(self.word2idx) >= self.max_size:\n",
    "                break\n",
    "            if word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "        \n",
    "        print(f\"Vocabulary built: {len(self)} tokens (min_freq={self.min_freq})\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simple word-level tokenization.\n",
    "        Lowercase and split on whitespace.\n",
    "        \"\"\"\n",
    "        return text.lower().strip().split()\n",
    "    \n",
    "    def numericalize(self, sentence: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a sentence to a list of indices.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Input sentence string\n",
    "            add_special_tokens: Whether to add <bos> and <eos>\n",
    "            \n",
    "        Returns:\n",
    "            List of token indices\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(sentence)\n",
    "        indices = [self.word2idx.get(token, self.unk_idx) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices = [self.bos_idx] + indices + [self.eos_idx]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: List[int], skip_special: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Convert indices back to a sentence string.\n",
    "        \n",
    "        Args:\n",
    "            indices: List of token indices\n",
    "            skip_special: Whether to skip special tokens\n",
    "            \n",
    "        Returns:\n",
    "            Decoded sentence string\n",
    "        \"\"\"\n",
    "        special_indices = {self.pad_idx, self.bos_idx, self.eos_idx}\n",
    "        tokens = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            if skip_special and idx in special_indices:\n",
    "                continue\n",
    "            if idx == self.eos_idx and skip_special:\n",
    "                break\n",
    "            tokens.append(self.idx2word.get(idx, self.UNK_TOKEN))\n",
    "        \n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77dffb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building source (English) vocabulary...\n",
      "Vocabulary built: 12801 tokens (min_freq=2)\n",
      "\n",
      "Building target (French) vocabulary...\n",
      "Vocabulary built: 15084 tokens (min_freq=2)\n",
      "\n",
      "Source vocabulary size: 12801\n",
      "Target vocabulary size: 15084\n"
     ]
    }
   ],
   "source": [
    "# Build vocabularies from training data\n",
    "print(\"Building source (English) vocabulary...\")\n",
    "src_sentences = [ex[\"translation\"][\"en\"] for ex in train_ds]\n",
    "src_vocab = Vocabulary(min_freq=Config.MIN_FREQ, max_size=Config.MAX_VOCAB_SIZE)\n",
    "src_vocab.build_vocab(src_sentences)\n",
    "\n",
    "print(\"\\nBuilding target (French) vocabulary...\")\n",
    "tgt_sentences = [ex[\"translation\"][\"fr\"] for ex in train_ds]\n",
    "tgt_vocab = Vocabulary(min_freq=Config.MIN_FREQ, max_size=Config.MAX_VOCAB_SIZE)\n",
    "tgt_vocab.build_vocab(tgt_sentences)\n",
    "\n",
    "print(f\"\\nSource vocabulary size: {len(src_vocab)}\")\n",
    "print(f\"Target vocabulary size: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c41fb",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader\n",
    "\n",
    "Create PyTorch Dataset and DataLoader with proper padding and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdcc7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 157\n",
      "Val batches: 16\n",
      "Test batches: 16\n",
      "\n",
      "Sample batch shapes:\n",
      "  src_batch: torch.Size([64, 50])\n",
      "  src_lengths: torch.Size([64])\n",
      "  tgt_input: torch.Size([64, 49])\n",
      "  tgt_output: torch.Size([64, 49])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for translation pairs.\n",
    "    \n",
    "    Handles numericalization and sequence length limiting.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        hf_dataset, \n",
    "        src_vocab: Vocabulary, \n",
    "        tgt_vocab: Vocabulary,\n",
    "        max_len: int = Config.MAX_SEQ_LEN\n",
    "    ):\n",
    "        self.data = hf_dataset\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            src_tensor: Source (English) token indices\n",
    "            tgt_tensor: Target (French) token indices\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        src_text = example[\"translation\"][\"en\"]\n",
    "        tgt_text = example[\"translation\"][\"fr\"]\n",
    "        \n",
    "        # Numericalize\n",
    "        src_indices = self.src_vocab.numericalize(src_text)\n",
    "        tgt_indices = self.tgt_vocab.numericalize(tgt_text)\n",
    "        \n",
    "        # Truncate if necessary (keeping <bos> and <eos>)\n",
    "        if len(src_indices) > self.max_len:\n",
    "            src_indices = src_indices[:self.max_len-1] + [self.src_vocab.eos_idx]\n",
    "        if len(tgt_indices) > self.max_len:\n",
    "            tgt_indices = tgt_indices[:self.max_len-1] + [self.tgt_vocab.eos_idx]\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader.\n",
    "    \n",
    "    Pads sequences and returns:\n",
    "        - src_batch: [batch_size, max_src_len] padded source sequences\n",
    "        - src_lengths: [batch_size] original source lengths\n",
    "        - tgt_input: [batch_size, max_tgt_len] target input (for teacher forcing)\n",
    "        - tgt_output: [batch_size, max_tgt_len] target output (shifted by 1)\n",
    "    \"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    \n",
    "    # Get lengths before padding\n",
    "    src_lengths = torch.tensor([len(s) for s in src_seqs])\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_batch = pad_sequence(src_seqs, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_batch = pad_sequence(tgt_seqs, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    \n",
    "    # For decoder: input is tgt[:-1], output is tgt[1:]\n",
    "    # Input starts with <bos>, output ends with <eos>\n",
    "    tgt_input = tgt_batch[:, :-1]\n",
    "    tgt_output = tgt_batch[:, 1:]\n",
    "    \n",
    "    return src_batch, src_lengths, tgt_input, tgt_output\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(train_ds, src_vocab, tgt_vocab)\n",
    "val_dataset = TranslationDataset(val_ds, src_vocab, tgt_vocab)\n",
    "test_dataset = TranslationDataset(test_ds, src_vocab, tgt_vocab)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if Config.DEVICE.type == \"cuda\" else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify batch structure\n",
    "sample_batch = next(iter(train_loader))\n",
    "src_batch, src_lengths, tgt_input, tgt_output = sample_batch\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  src_batch: {src_batch.shape}\")\n",
    "print(f\"  src_lengths: {src_lengths.shape}\")\n",
    "print(f\"  tgt_input: {tgt_input.shape}\")\n",
    "print(f\"  tgt_output: {tgt_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be02e16",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Implementing the encoder-decoder LSTM architecture from Sutskever et al., 2014:\n",
    "\n",
    "**Encoder:**\n",
    "- Embedding layer for source tokens\n",
    "- 4-layer LSTM (as specified in the paper)\n",
    "- Returns final hidden/cell states to initialize decoder\n",
    "\n",
    "**Decoder:**\n",
    "- Embedding layer for target tokens  \n",
    "- 4-layer LSTM initialized from encoder states\n",
    "- Linear layer to project hidden states to vocabulary logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f75a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Encoder for Seq2Seq model.\n",
    "    \n",
    "    As per Sutskever et al., 2014:\n",
    "    - Uses 4 stacked LSTM layers\n",
    "    - Processes source sequence and returns final hidden states\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        pad_idx: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=False  # Unidirectional as in the paper\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        src: torch.Tensor,\n",
    "        src_lengths: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size, src_len] source token indices\n",
    "            src_lengths: [batch_size] original sequence lengths\n",
    "            \n",
    "        Returns:\n",
    "            outputs: [batch_size, src_len, hidden_dim] encoder outputs\n",
    "            (hidden, cell): Final hidden and cell states for each layer\n",
    "        \"\"\"\n",
    "        # Embed tokens: [batch_size, src_len, embedding_dim]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # Pack for efficiency with variable length sequences\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, \n",
    "            src_lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack outputs\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        \n",
    "        # hidden: [num_layers, batch_size, hidden_dim]\n",
    "        # cell: [num_layers, batch_size, hidden_dim]\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Decoder for Seq2Seq model.\n",
    "    \n",
    "    As per Sutskever et al., 2014:\n",
    "    - Uses 4 stacked LSTM layers\n",
    "    - Initialized with encoder's final hidden states\n",
    "    - Generates one token at a time\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        pad_idx: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_token: torch.Tensor,\n",
    "        hidden: torch.Tensor,\n",
    "        cell: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Single step of decoding.\n",
    "        \n",
    "        Args:\n",
    "            input_token: [batch_size, 1] current input token\n",
    "            hidden: [num_layers, batch_size, hidden_dim]\n",
    "            cell: [num_layers, batch_size, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            prediction: [batch_size, vocab_size] logits for next token\n",
    "            hidden: Updated hidden state\n",
    "            cell: Updated cell state\n",
    "        \"\"\"\n",
    "        # Embed input: [batch_size, 1, embedding_dim]\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        # LSTM step: output is [batch_size, 1, hidden_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # Project to vocabulary: [batch_size, vocab_size]\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence to Sequence model combining Encoder and Decoder.\n",
    "    \n",
    "    Implements teacher forcing during training as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: Encoder,\n",
    "        decoder: Decoder,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_lengths: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        teacher_forcing_ratio: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with teacher forcing.\n",
    "        \n",
    "        Args:\n",
    "            src: [batch_size, src_len] source sequences\n",
    "            src_lengths: [batch_size] source lengths\n",
    "            tgt: [batch_size, tgt_len] target sequences (input)\n",
    "            teacher_forcing_ratio: Probability of using ground truth\n",
    "            \n",
    "        Returns:\n",
    "            outputs: [batch_size, tgt_len, vocab_size] predictions\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        _, (hidden, cell) = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # First input to decoder is <bos> token (first column of tgt)\n",
    "        input_token = tgt[:, 0:1]  # [batch_size, 1]\n",
    "        \n",
    "        for t in range(tgt_len):\n",
    "            # Decoder step\n",
    "            prediction, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs[:, t, :] = prediction\n",
    "            \n",
    "            # Teacher forcing: use ground truth or predicted token\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get top prediction\n",
    "            top1 = prediction.argmax(1, keepdim=True)  # [batch_size, 1]\n",
    "            \n",
    "            # Next input: ground truth if teacher forcing, else prediction\n",
    "            if t < tgt_len - 1:\n",
    "                input_token = tgt[:, t+1:t+2] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7254d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 30,638,060 trainable parameters\n",
      "\n",
      "Encoder parameters: 11,157,760\n",
      "Decoder parameters: 19,480,300\n"
     ]
    }
   ],
   "source": [
    "# Initialize model components\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(src_vocab),\n",
    "    embedding_dim=Config.EMBEDDING_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    num_layers=Config.NUM_LAYERS,\n",
    "    dropout=Config.DROPOUT,\n",
    "    pad_idx=src_vocab.pad_idx\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(tgt_vocab),\n",
    "    embedding_dim=Config.EMBEDDING_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    num_layers=Config.NUM_LAYERS,\n",
    "    dropout=Config.DROPOUT,\n",
    "    pad_idx=tgt_vocab.pad_idx\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, Config.DEVICE).to(Config.DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")\n",
    "print(f\"\\nEncoder parameters: {count_parameters(encoder):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(decoder):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b83221",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Training with:\n",
    "- Cross-entropy loss (ignoring padding)\n",
    "- Adam optimizer\n",
    "- Gradient clipping (as recommended in the paper to prevent exploding gradients)\n",
    "- Teacher forcing with configurable ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c54264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: Seq2Seq,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    clip: float,\n",
    "    teacher_forcing_ratio: float,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for src_batch, src_lengths, tgt_input, tgt_output in progress_bar:\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # output: [batch_size, tgt_len, vocab_size]\n",
    "        output = model(src_batch, src_lengths, tgt_input, teacher_forcing_ratio)\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        # output: [batch_size * tgt_len, vocab_size]\n",
    "        # tgt_output: [batch_size * tgt_len]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (as in the paper)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: Seq2Seq,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_batch, src_lengths, tgt_input, tgt_output in dataloader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_input = tgt_input.to(device)\n",
    "            tgt_output = tgt_output.to(device)\n",
    "            \n",
    "            # Forward pass (no teacher forcing during evaluation)\n",
    "            output = model(src_batch, src_lengths, tgt_input, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Reshape for loss\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f06b1e",
   "metadata": {},
   "source": [
    "## 6. Inference and Evaluation\n",
    "\n",
    "Implement:\n",
    "- Greedy decoding for inference\n",
    "- BLEU score computation using sacrebleu\n",
    "- Translation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08fc73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "    model: Seq2Seq,\n",
    "    src_sentence: str,\n",
    "    src_vocab: Vocabulary,\n",
    "    tgt_vocab: Vocabulary,\n",
    "    max_len: int = Config.MAX_SEQ_LEN,\n",
    "    device: torch.device = Config.DEVICE\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Greedy decoding for a single sentence.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Seq2Seq model\n",
    "        src_sentence: Source sentence string\n",
    "        src_vocab: Source vocabulary\n",
    "        tgt_vocab: Target vocabulary\n",
    "        max_len: Maximum output length\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        Decoded translation string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Numericalize source sentence\n",
    "        src_indices = src_vocab.numericalize(src_sentence)\n",
    "        src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)  # [1, src_len]\n",
    "        src_lengths = torch.tensor([len(src_indices)])\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, cell) = model.encoder(src_tensor, src_lengths)\n",
    "        \n",
    "        # Start with <bos> token\n",
    "        input_token = torch.tensor([[tgt_vocab.bos_idx]]).to(device)\n",
    "        \n",
    "        output_indices = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # Decode one step\n",
    "            prediction, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "            \n",
    "            # Get top prediction\n",
    "            top1 = prediction.argmax(1).item()\n",
    "            \n",
    "            # Stop if <eos>\n",
    "            if top1 == tgt_vocab.eos_idx:\n",
    "                break\n",
    "                \n",
    "            output_indices.append(top1)\n",
    "            \n",
    "            # Next input\n",
    "            input_token = torch.tensor([[top1]]).to(device)\n",
    "        \n",
    "        # Decode to string\n",
    "        return tgt_vocab.decode(output_indices, skip_special=True)\n",
    "\n",
    "\n",
    "def compute_bleu(\n",
    "    model: Seq2Seq,\n",
    "    dataset,\n",
    "    src_vocab: Vocabulary,\n",
    "    tgt_vocab: Vocabulary,\n",
    "    num_samples: int = 100,\n",
    "    device: torch.device = Config.DEVICE\n",
    ") -> Tuple[float, List[Tuple[str, str, str]]]:\n",
    "    \"\"\"\n",
    "    Compute BLEU score on a subset of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: HuggingFace dataset\n",
    "        src_vocab: Source vocabulary\n",
    "        tgt_vocab: Target vocabulary\n",
    "        num_samples: Number of samples to evaluate\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        BLEU score and list of (source, reference, hypothesis) tuples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    examples = []\n",
    "    \n",
    "    # Sample indices\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Computing BLEU\"):\n",
    "        example = dataset[idx]\n",
    "        src_text = example[\"translation\"][\"en\"]\n",
    "        ref_text = example[\"translation\"][\"fr\"]\n",
    "        \n",
    "        # Generate translation\n",
    "        hyp_text = greedy_decode(model, src_text, src_vocab, tgt_vocab, device=device)\n",
    "        \n",
    "        hypotheses.append(hyp_text)\n",
    "        references.append(ref_text)\n",
    "        examples.append((src_text, ref_text, hyp_text))\n",
    "    \n",
    "    # Compute BLEU score using sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
    "    \n",
    "    return bleu.score, examples\n",
    "\n",
    "\n",
    "def show_translations(examples: List[Tuple[str, str, str]], num: int = 5):\n",
    "    \"\"\"Display sample translations.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE TRANSLATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (src, ref, hyp) in enumerate(examples[:num]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Source (EN):     {src}\")\n",
    "        print(f\"Reference (FR):  {ref}\")\n",
    "        print(f\"Hypothesis (FR): {hyp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ca99c",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Train the model and track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Configuration: 10 epochs, batch size 64\n",
      "Teacher forcing ratio: 0.5\n",
      "Gradient clipping: 5.0\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa3bd4d2e944f4e800e25054937690f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Configuration: {Config.EPOCHS} epochs, batch size {Config.BATCH_SIZE}\")\n",
    "print(f\"Teacher forcing ratio: {Config.TEACHER_FORCING_RATIO}\")\n",
    "print(f\"Gradient clipping: {Config.CLIP_GRAD}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(1, Config.EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, criterion,\n",
    "        Config.CLIP_GRAD, Config.TEACHER_FORCING_RATIO, Config.DEVICE\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, val_loader, criterion, Config.DEVICE)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Compute perplexity\n",
    "    train_ppl = math.exp(train_loss)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} (PPL: {train_ppl:.2f}) | \"\n",
    "          f\"Val Loss: {val_loss:.4f} (PPL: {val_ppl:.2f})\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbc3e9",
   "metadata": {},
   "source": [
    "### Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train')\n",
    "axes[0].plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Perplexity plot\n",
    "train_ppls = [math.exp(l) for l in train_losses]\n",
    "val_ppls = [math.exp(l) for l in val_losses]\n",
    "axes[1].plot(range(1, len(train_ppls) + 1), train_ppls, 'b-', label='Train')\n",
    "axes[1].plot(range(1, len(val_ppls) + 1), val_ppls, 'r-', label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Training and Validation Perplexity')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4fe2c",
   "metadata": {},
   "source": [
    "## 8. Test Set Evaluation\n",
    "\n",
    "Load the best model and evaluate on the test set with BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = evaluate(model, test_loader, criterion, Config.DEVICE)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_ppl:.2f}\")\n",
    "\n",
    "# Compute BLEU score on test set\n",
    "print(\"\\nComputing BLEU score on test set...\")\n",
    "bleu_score, examples = compute_bleu(\n",
    "    model, test_ds, src_vocab, tgt_vocab, \n",
    "    num_samples=500,  # Evaluate on 500 samples for speed\n",
    "    device=Config.DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nBLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# Show some translations\n",
    "show_translations(examples, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b957b3",
   "metadata": {},
   "source": [
    "## 9. Analysis and Comparison to Paper\n",
    "\n",
    "### Comparison with Sutskever et al., 2014\n",
    "\n",
    "The original paper achieved a BLEU score of **34.81** on the WMT'14 En→Fr task with:\n",
    "- Full WMT'14 dataset (~12M sentence pairs)\n",
    "- 4-layer LSTM with 1000 hidden units per layer\n",
    "- 1000-dimensional embeddings\n",
    "- Training on 8 GPUs for several days\n",
    "- Beam search decoding with beam size 12\n",
    "\n",
    "**Our implementation differs due to compute constraints:**\n",
    "- 10,000 training examples (vs ~12M)\n",
    "- 512 hidden units (vs 1000)  \n",
    "- 256-dimensional embeddings (vs 1000)\n",
    "- Greedy decoding (vs beam search)\n",
    "- Much shorter training time\n",
    "\n",
    "These factors explain the lower BLEU score. The reduced dataset size is the primary limiting factor, as neural translation models are highly data-hungry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  - Encoder/Decoder layers: {Config.NUM_LAYERS}\")\n",
    "print(f\"  - Hidden dimension: {Config.HIDDEN_DIM}\")\n",
    "print(f\"  - Embedding dimension: {Config.EMBEDDING_DIM}\")\n",
    "print(f\"  - Total parameters: {count_parameters(model):,}\")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Training examples: {Config.TRAIN_SIZE}\")\n",
    "print(f\"  - Validation examples: {Config.VAL_SIZE}\")\n",
    "print(f\"  - Test examples: {Config.TEST_SIZE}\")\n",
    "print(f\"  - Source vocabulary size: {len(src_vocab)}\")\n",
    "print(f\"  - Target vocabulary size: {len(tgt_vocab)}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {Config.EPOCHS}\")\n",
    "print(f\"  - Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"  - Learning rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"  - Teacher forcing ratio: {Config.TEACHER_FORCING_RATIO}\")\n",
    "print(f\"  - Gradient clipping: {Config.CLIP_GRAD}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - Test loss: {test_loss:.4f}\")\n",
    "print(f\"  - Test perplexity: {test_ppl:.2f}\")\n",
    "print(f\"  - BLEU score: {bleu_score:.2f}\")\n",
    "\n",
    "print(f\"\\nComparison to Paper (Sutskever et al., 2014):\")\n",
    "print(f\"  - Paper BLEU: 34.81 (on full WMT'14, beam search)\")\n",
    "print(f\"  - Our BLEU: {bleu_score:.2f} (on 10k subset, greedy decoding)\")\n",
    "print(f\"  - Key differences: ~1200x less data, ~4x smaller model, greedy decoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e59410",
   "metadata": {},
   "source": [
    "## 10. Interactive Translation\n",
    "\n",
    "Test the model with custom sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str) -> str:\n",
    "    \"\"\"Translate an English sentence to French.\"\"\"\n",
    "    model.eval()\n",
    "    translation = greedy_decode(model, sentence, src_vocab, tgt_vocab, device=Config.DEVICE)\n",
    "    return translation\n",
    "\n",
    "# Test with some example sentences\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love deep learning.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"What is your name?\",\n",
    "    \"Thank you very much.\",\n",
    "]\n",
    "\n",
    "print(\"Custom translations:\")\n",
    "print(\"-\" * 60)\n",
    "for sentence in test_sentences:\n",
    "    translation = translate(sentence)\n",
    "    print(f\"EN: {sentence}\")\n",
    "    print(f\"FR: {translation}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
